{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rdflib\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "WcftqzBGVSGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b2fb65b-173f-4828-db1c-04563b225ee9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.2.0-py3-none-any.whl (500 kB)\n",
            "\u001b[K     |████████████████████████████████| 500 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from rdflib) (57.4.0)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 244 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from rdflib) (3.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from isodate->rdflib) (1.15.0)\n",
            "Installing collected packages: isodate, rdflib\n",
            "Successfully installed isodate-0.6.1 rdflib-6.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 43.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 42.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import rdflib\n",
        "from rdflib.namespace import Namespace, RDF, RDFS, XSD\n",
        "from rdflib.term import URIRef, Literal\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from transformers import pipeline\n",
        "import csv\n",
        "from sklearn.metrics import pairwise_distances"
      ],
      "metadata": {
        "id": "yU_EUZObVHFL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WDT = Namespace('http://www.wikidata.org/prop/direct/')\n",
        "\n",
        "\n",
        "def dataloader():\n",
        "    print('############# Start loading data #############')\n",
        "\n",
        "    ner_pipeline = pipeline('ner', model='dbmdz/bert-large-cased-finetuned-conll03-english')\n",
        "\n",
        "    # Build the graph\n",
        "    graph = rdflib.Graph()\n",
        "    graph.parse('drive/MyDrive/14_graph.nt', format='turtle')\n",
        "\n",
        "    # Load embedding dictionaries\n",
        "    with open('drive/MyDrive/entity_ids.del', 'r') as ifile:\n",
        "        ent2id = {rdflib.term.URIRef(ent): int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
        "        id2ent = {v: k for k, v in ent2id.items()}\n",
        "    with open('drive/MyDrive/relation_ids.del', 'r') as ifile:\n",
        "        rel2id = {rdflib.term.URIRef(rel): int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
        "        id2rel = {v: k for k, v in rel2id.items()}\n",
        "\n",
        "    triple_df = pd.read_csv('drive/MyDrive/14_graph.tsv', sep='\\t', names=[\"entity1\", \"relation\", \"entity2\"])\n",
        "    entity_emb = np.load('drive/MyDrive/entity_embeds.npy')\n",
        "    relation_emb = np.load('drive/MyDrive/relation_embeds.npy')\n",
        "\n",
        "    ent2imb = {str(ent): str(imb) for ent, imb in graph.subject_objects(WDT.P345)}\n",
        "\n",
        "    ent2lbl = {ent: str(lbl) for ent, lbl in graph.subject_objects(RDFS.label)}\n",
        "    lbl2ent = {lbl: ent for ent, lbl in ent2lbl.items()}\n",
        "\n",
        "    # Load multimedia dataset\n",
        "    f = open('drive/MyDrive/images.json')\n",
        "    mediadata = json.load(f)\n",
        "\n",
        "    # Load crowdsource dataset\n",
        "    crowd_df = pd.read_csv('drive/MyDrive/crowd_data.tsv', sep='\\t')\n",
        "\n",
        "    print('Data loading done.')\n",
        "    return graph, ent2id, id2ent, rel2id, id2rel, ent2lbl, lbl2ent, triple_df, entity_emb, \\\n",
        "           relation_emb, ent2imb, mediadata, crowd_df, ner_pipeline"
      ],
      "metadata": {
        "id": "yuTmN6IYVkIE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4yaUYxC5VDqH"
      },
      "outputs": [],
      "source": [
        "def questionprocessor(question, ner_pipeline):\n",
        "    if question.find('VI -') != -1:\n",
        "        question = question.replace('-', '–')\n",
        "\n",
        "    if question.find('ecommend') != -1:\n",
        "        qtype = 'Recommend'\n",
        "    elif (question.find('picture') != -1) or (question.find('like') != -1) or (question.find('figure') != -1):\n",
        "        qtype = 'Multimedia'\n",
        "    else:\n",
        "        qtype = 'KG & Embedding'\n",
        "\n",
        "    if question.find('of') != -1:\n",
        "        sub1 = \"of \"\n",
        "        sub2 = \" ?\"\n",
        "        idx1 = question.find(sub1)\n",
        "        idx2 = question.find(sub2)\n",
        "        movie = question[idx1 + len(sub1): idx2]\n",
        "        return qtype, movie\n",
        "    else:\n",
        "        movies = []\n",
        "        entities = ner_pipeline(question, aggregation_strategy=\"simple\")\n",
        "        for entity in entities:\n",
        "            movies.append(entity['word'])\n",
        "\n",
        "    if question.find('ecommend') != -1:\n",
        "        return qtype, movies\n",
        "    else:\n",
        "        return qtype, movies[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def factual(question, graph, movies, ent2lbl, lbl2ent, ans_df):\n",
        "    try:\n",
        "        WD = Namespace('http://www.wikidata.org/entity/')\n",
        "        WDT = Namespace('http://www.wikidata.org/prop/direct/')\n",
        "\n",
        "        if question.find('of') != -1:\n",
        "            sub1 = \" the \"\n",
        "            sub2 = \" of \"\n",
        "            idx1 = question.find(sub1)\n",
        "            idx2 = question.find(sub2)\n",
        "            relation = question[idx1 + len(sub1): idx2]\n",
        "            print('The relation is', relation)\n",
        "\n",
        "        if question.find('by') != -1:\n",
        "            sub1 = \" the \"\n",
        "            sub2 = \" by \"\n",
        "            idx1 = question.find(sub1)\n",
        "            idx2 = question.find(sub2)\n",
        "            relation = question[idx1 + len(sub1): idx2]\n",
        "            print('The relation is', relation)\n",
        "\n",
        "        if question.find('direct') != -1:\n",
        "            relation = 'director'\n",
        "\n",
        "        query_relURI = '''\n",
        "            SELECT ?rel WHERE{{\n",
        "                ?rel rdfs:label \"{}\"@en.\n",
        "                }}'''.format(relation) \n",
        "\n",
        "        relURIList = list(graph.query(query_relURI))\n",
        "        for idx, relURI in enumerate(relURIList):\n",
        "            rel_tmp = relURI[0].n3()\n",
        "            if WDT in rel_tmp:\n",
        "                rel = rel_tmp\n",
        "\n",
        "        mov = lbl2ent[movies].n3()\n",
        "\n",
        "        ent1 = re.sub('<|>', '', mov)\n",
        "        ent2 = re.sub('<|>', '', rel)\n",
        "        crowd_idx1 = 'wd:'+re.findall(r'http://www.wikidata.org/entity/(.*)', ent1)[0]\n",
        "        crowd_idx2 = 'wdt:'+re.findall(r'http://www.wikidata.org/prop/direct/(.*)', ent2)[0]\n",
        "\n",
        "        if (crowd_idx1 in ans_df['Input1ID'].values) & (crowd_idx2 in ans_df['Input2ID'].values):\n",
        "            tmp = ans_df.loc[ans_df['Input1ID']==crowd_idx1]\n",
        "            ans = tmp['Input3ID'].values[0]\n",
        "            if ans.startswith('wd:'):\n",
        "                ans = ent2lbl[rdflib.term.URIRef(ent1)]\n",
        "            message = 'The answer is '+ans+', according to the crowd, who had an inter-rater agreement of: '\\\n",
        "            +str(tmp['Kappa'].values[0])+ \\\n",
        "            '.\\nThe answer distribution is: '+str(tmp['Correct'].values[0])+\\\n",
        "            ' support vote and '+str(3-tmp['Correct'].values[0])+' reject vote.'\n",
        "\n",
        "        else:\n",
        "            idxs = triple_df[(triple_df['entity1'] == mov) & (triple_df['relation'] == rel)].index.values\n",
        "\n",
        "            entity2 = triple_df['entity2'].iloc[idxs[0]]\n",
        "            entity2 = re.sub('<|>', '', entity2)\n",
        "            entity2_lbl = ent2lbl[rdflib.term.URIRef(entity2)]\n",
        "\n",
        "            answers = [\n",
        "                'KG: I think it is ' + entity2_lbl+'\\n',\n",
        "                'KG: ' + entity2_lbl + ' is the ' + relation + ' of ' + movies+'\\n'\n",
        "            ]\n",
        "\n",
        "            message = random.choice(answers)\n",
        "    except:\n",
        "        message = 'KG: No answer.\\n'\n",
        "    return message"
      ],
      "metadata": {
        "id": "-a-wFnxdVnDs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding(question, graph, movies, ent2id, id2ent, rel2id, id2rel, triple_df, entity_emb, relation_emb):\n",
        "    try:\n",
        "        WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
        "\n",
        "        if question.find('the') != -1:\n",
        "            sub1 = \" the \"\n",
        "            sub2 = \" of \"\n",
        "            idx1 = question.find(sub1)\n",
        "            idx2 = question.find(sub2)\n",
        "            relation = question[idx1 + len(sub1): idx2]\n",
        "            print('relation is ', relation)\n",
        "\n",
        "        if question.find('direct') != -1:\n",
        "            relation = 'director'\n",
        "\n",
        "        query_relURI = '''\n",
        "            SELECT ?rel WHERE{{\n",
        "                ?rel rdfs:label \"{}\"@en.\n",
        "                }}'''.format(relation)\n",
        "\n",
        "        relURI = []\n",
        "        relURIList = list(graph.query(query_relURI))\n",
        "        for idx, relURI in enumerate(relURIList):\n",
        "            tmp = str(relURI[0])\n",
        "            if WDT in tmp:\n",
        "                rel = tmp\n",
        "\n",
        "        mov = lbl2ent[movies].n3()\n",
        "\n",
        "        rel_id = rel2id[rdflib.term.URIRef(rel)]\n",
        "        mov_id = ent2id[rdflib.term.URIRef(re.sub('<|>','',mov))]\n",
        "\n",
        "        topN = 3\n",
        "        rel_emb = np.atleast_2d(relation_emb[rel_id])\n",
        "        rel_dist = pairwise_distances(rel_emb, relation_emb)\n",
        "        relation2 = []\n",
        "        for idx in rel_dist.argsort().reshape(-1)[:3]:\n",
        "            relation2.append(str(id2rel[idx].n3()))\n",
        "\n",
        "        rel = '<'+rel+'>'\n",
        "        idxs = 0\n",
        "        idxs = triple_df[(triple_df['entity1'] == mov) & (triple_df['relation'] == rel)].index.values\n",
        "        length = len(idxs)\n",
        "        if length == 0:\n",
        "            idxs = triple_df[(triple_df['entity1'] == mov) & (triple_df['relation'] == relation2[1])].index.values\n",
        "\n",
        "        entity2 = triple_df['entity2'].iloc[idxs[0]]\n",
        "        entity2 = re.sub('<|>','',entity2)\n",
        "        entity2_id = ent2id[rdflib.term.URIRef(entity2)]\n",
        "\n",
        "        # TransE\n",
        "        topN = 3\n",
        "        emb = np.atleast_2d(entity_emb[entity2_id])\n",
        "        dist = pairwise_distances(emb, entity_emb)\n",
        "        entity2 = []\n",
        "        answers = []\n",
        "        for idx in dist.argsort().reshape(-1)[:3]:\n",
        "            answers.append(ent2lbl[id2ent[idx]])\n",
        "        \n",
        "        message = '\\n----------The answers suggested by embeddings are '+answers[0]+', '+answers[1]+', '+answers[2]\n",
        "    except:\n",
        "        message = '\\n----------Embedding: No answer.'\n",
        "\n",
        "    return message"
      ],
      "metadata": {
        "id": "GVaMo0JKVsfm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multimedia(question, graph, movies, mediadata, ner_pipeline):\n",
        "    try:\n",
        "        WDT = Namespace('http://www.wikidata.org/prop/direct/')\n",
        "        lbl2ent = {str(lbl): str(ent) for ent, lbl in graph.subject_objects(RDFS.label)}\n",
        "        ent2imb = {str(ent): str(imb) for ent, imb in graph.subject_objects(WDT.P345)}\n",
        "\n",
        "        entities = ner_pipeline(question, aggregation_strategy=\"simple\")\n",
        "        for entity in entities:\n",
        "            lbl = entity['word']\n",
        "\n",
        "        ent = lbl2ent[lbl]\n",
        "        imb = ent2imb[ent]\n",
        "\n",
        "        if imb[:2] == 'tt':\n",
        "            for item in mediadata:\n",
        "                if imb in item['movie']:\n",
        "                    message = item['img']\n",
        "                    break\n",
        "        elif imb[:2] == 'nm':\n",
        "            for item in mediadata:\n",
        "                if (imb in item['cast']) & (len(item['cast'])==1):\n",
        "                    message = item['img']\n",
        "                    break\n",
        "        else:\n",
        "            message = 'Not a movie or human.'\n",
        "    except:\n",
        "        message = 'No answer.'\n",
        "        \n",
        "    return 'image:'+message"
      ],
      "metadata": {
        "id": "4z11LYUwehoq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend(question, graph, movies, ent2lbl, triple_df):\n",
        "    try:\n",
        "        mov_list = []\n",
        "\n",
        "        mov_list = []\n",
        "        for mov in movies:\n",
        "            mov_lbl = [str(s) for s,  in graph.query('''\n",
        "                SELECT ?movie WHERE {\n",
        "                    ?movie rdfs:label '%s'@en .\n",
        "                }'''%mov)]\n",
        "            if len(mov_lbl)!= 0:\n",
        "                mov_list.append(mov_lbl)\n",
        "        \n",
        "        for i in range(len(mov_list)):\n",
        "            for j in range(len(mov_list[i])):\n",
        "                mov_list[i][j] = '<' + mov_list[i][j] + '>'\n",
        "\n",
        "        dfs = []\n",
        "        for i in range(len(mov_list)):\n",
        "            df = []\n",
        "            df = triple_df.loc[triple_df['entity1'].isin(mov_list[i])]\n",
        "            dfs.append(df)\n",
        "\n",
        "        for i in range(1, len(mov_list)):\n",
        "            dfs[i] = pd.merge(dfs[i-1], dfs[i], on=[\"relation\", \"entity2\"])\n",
        "        \n",
        "        rel_df = dfs[len(mov_list)-1]\n",
        "        rel_df = rel_df.drop_duplicates(subset=['relation', 'entity2'])\n",
        "        entity2 = rel_df['entity2'].values.tolist()\n",
        "\n",
        "        common = []\n",
        "        for ent in entity2:\n",
        "            if '<' in ent:\n",
        "                ent = re.sub('<|>','',ent)\n",
        "                lbl = ent2lbl[rdflib.term.URIRef(ent)]\n",
        "            else:\n",
        "                lbl = ent\n",
        "            common.append(lbl)\n",
        "\n",
        "        entity1 = []\n",
        "        entity1 = triple_df['entity1'].loc[triple_df['entity2'].isin(entity2)]  \n",
        "        entity1 = entity1.value_counts()[len(mov_list):len(mov_list)+3].index.tolist()  \n",
        "\n",
        "        answers = []\n",
        "        for ent in entity1:\n",
        "            ent = re.sub('<|>','',ent)\n",
        "            lbl = ent2lbl[rdflib.term.URIRef(ent)]\n",
        "            answers.append(lbl)\n",
        "\n",
        "        message = answers[0]+', '+answers[1]+', '+answers[2]\n",
        "    except:\n",
        "        message = 'No answer.'\n",
        "\n",
        "    return message"
      ],
      "metadata": {
        "id": "uFhgBp5Zi0fp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crowdsource(crowd_df):\n",
        "    crowd_df.drop(['Title','Reward','AssignmentId','AssignmentStatus'], inplace=True, axis=1)\n",
        "    crowd_df['LifetimeApprovalRate'] = crowd_df['LifetimeApprovalRate'].str.rstrip('%').astype('float') / 100.0\n",
        "    crowd_df = crowd_df.loc[(crowd_df['WorkTimeInSeconds'] >= 50) & (crowd_df['LifetimeApprovalRate'] >= 0.7)]\n",
        "    crowd_df.drop(['WorkerId','WorkTimeInSeconds','LifetimeApprovalRate'], inplace=True, axis=1)\n",
        "    ans_df = crowd_df.groupby(['HITId']).first()\n",
        "\n",
        "    rate = []\n",
        "    ans_df['Correct'] = None\n",
        "\n",
        "    for i in range(1, len(ans_df)+1):\n",
        "        # Get the specific group\n",
        "        df = crowd_df.loc[crowd_df['HITId']== i]\n",
        "\n",
        "        corr_count = int(df['AnswerID'][df['AnswerID']==1].count())\n",
        "        incorr_count = int(df['AnswerID'][df['AnswerID']==2].count())\n",
        "        ans_df['Correct'][i] = corr_count\n",
        "\n",
        "\n",
        "        rate.append([corr_count, incorr_count])\n",
        "        if (corr_count < incorr_count):\n",
        "            ans_df['AnswerLabel'][i] = 'INCORRECT'\n",
        "\n",
        "            fixValueLoc = df['FixValue'].first_valid_index()\n",
        "            fixPositionLoc = df['FixPosition'].first_valid_index()\n",
        "\n",
        "            if fixValueLoc is not None:\n",
        "\n",
        "                fixPosition = crowd_df['FixPosition'][fixPositionLoc]\n",
        "                fixValue = crowd_df['FixValue'][fixValueLoc]\n",
        "\n",
        "                if fixPosition == 'Subject':\n",
        "                    if fixValue.startswith('Q'):\n",
        "                        ans_df['Input1ID'][i] = 'wd:'+fixValue\n",
        "                    else:\n",
        "                        ans_df['Input1ID'][i] = fixValue\n",
        "                elif fixPosition == 'Predicate':\n",
        "                    if fixValue.startswith('P'):\n",
        "                        ans_df['Input2ID'][i] = 'wdt:'+fixValue\n",
        "                    else:\n",
        "                        ans_df['Input2ID'][i] = fixValue\n",
        "                else:\n",
        "                    if fixValue.startswith('Q'):\n",
        "                        ans_df['Input3ID'][i] = 'wd:'+fixValue\n",
        "                    else:\n",
        "                        ans_df['Input3ID'][i] = fixValue\n",
        "        \n",
        "        else:\n",
        "            ans_df['AnswerLabel'][i] = 'CORRECT'\n",
        "\n",
        "\n",
        "    def checkInput(rate, n):\n",
        "        \"\"\" \n",
        "        Check correctness of the input matrix\n",
        "        @param rate - ratings matrix\n",
        "        @return n - number of raters\n",
        "        @throws AssertionError \n",
        "        \"\"\"\n",
        "        N = len(rate)\n",
        "        k = len(rate[0])\n",
        "        assert all(len(rate[i]) == k for i in range(k)), \"Row length != #categories)\"\n",
        "        assert all(isinstance(rate[i][j], int) for i in range(N) for j in range(k)), \"Element not integer\" \n",
        "        assert all(sum(row) == n for row in rate), \"Sum of ratings != #raters)\"\n",
        "\n",
        "    def fleissKappa(rate,n):\n",
        "        \"\"\" \n",
        "        Computes the Kappa value\n",
        "        @param rate - ratings matrix containing number of ratings for each subject per category \n",
        "        [size - N X k where N = #subjects and k = #categories]\n",
        "        @param n - number of raters   \n",
        "        @return fleiss' kappa\n",
        "        \"\"\"\n",
        "\n",
        "        N = len(rate)\n",
        "        k = len(rate[0])\n",
        "        print(\"#raters = \", n, \", #subjects = \", N, \", #categories = \", k)\n",
        "        checkInput(rate, n)\n",
        "\n",
        "        #mean of the extent to which raters agree for the ith subject \n",
        "        PA = sum([(sum([i**2 for i in row])- n) / (n * (n - 1)) for row in rate])/N\n",
        "        print(\"PA = \", PA)\n",
        "        \n",
        "        # mean of squares of proportion of all assignments which were to jth category\n",
        "        PE = sum([j**2 for j in [sum([rows[i] for rows in rate])/(N*n) for i in range(k)]])\n",
        "        print(\"PE =\", PE)\n",
        "        \n",
        "        kappa = -float(\"inf\")\n",
        "        try:\n",
        "            kappa = (PA - PE) / (1 - PE)\n",
        "            kappa = float(\"{:.3f}\".format(kappa))\n",
        "        except ZeroDivisionError:\n",
        "            print(\"Expected agreement = 1\")\n",
        "\n",
        "        print(\"Fleiss' Kappa =\", kappa)\n",
        "        \n",
        "        return kappa\n",
        "\n",
        "    len1 = len(ans_df[ans_df['HITTypeId']=='7QT'])\n",
        "    len2 = len(ans_df[ans_df['HITTypeId']=='8QT'])\n",
        "    len3 = len(ans_df[ans_df['HITTypeId']=='9QT'])\n",
        "\n",
        "    rate1 = rate[:len1]\n",
        "    rate2 = rate[len1:len1+len2]\n",
        "    rate3 = rate[len1+len2:]\n",
        "\n",
        "    kappa1 = fleissKappa(rate1, 3)\n",
        "    kappa2 = fleissKappa(rate2, 3)\n",
        "    kappa3 = fleissKappa(rate3, 3)\n",
        "\n",
        "    ans_df['Kappa'] = None\n",
        "    ans_df['Kappa'][:len1] = kappa1\n",
        "    ans_df['Kappa'][len1:len1+len2] = kappa2\n",
        "    ans_df['Kappa'][len1+len2:] = kappa3\n",
        "    \n",
        "    return ans_df"
      ],
      "metadata": {
        "id": "wwIFNBUb9ZG_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph, ent2id, id2ent, rel2id, id2rel, ent2lbl, lbl2ent, triple_df, \\\n",
        "entity_emb, relation_emb, ent2imb, mediadata, crowd_df, ner_pipeline = dataloader()\n",
        "ans_df = crowdsource(crowd_df)"
      ],
      "metadata": {
        "id": "0-A37Jycl6b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatAgent(question):\n",
        "    qtype, movies = questionprocessor(question, ner_pipeline)\n",
        "\n",
        "    if qtype == 'KG & Embedding':\n",
        "        message = factual(question, graph, movies, ent2lbl, lbl2ent, ans_df)+\\\n",
        "        embedding(question, graph, movies, ent2id, id2ent, rel2id, id2rel, triple_df, entity_emb, relation_emb)\n",
        "    elif qtype == 'Multimedia':\n",
        "        message = multimedia(question, graph, movies, mediadata, ner_pipeline)\n",
        "    elif qtype == 'Recommend':\n",
        "        message = recommend(question, graph, movies, ent2lbl, triple_df)\n",
        "\n",
        "    return message"
      ],
      "metadata": {
        "id": "xbbvD9-lVpW0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "sGnIen5GdhYd",
        "outputId": "61176cba-9487-44e5-fce7-59bc36061c9c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "que = input()\n",
        "message = chatAgent(que)\n",
        "message"
      ],
      "metadata": {
        "id": "brz7SojcDRZZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "4d2c3c55-1f80-466c-cddf-99f2f18e0746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-2edef622352d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mque\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchatAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mque\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import atexit\n",
        "import getpass\n",
        "import requests  # install the package via \"pip install requests\"\n",
        "from collections import defaultdict\n",
        "\n",
        "# url of the speakeasy server\n",
        "url = 'https://speakeasy.ifi.uzh.ch'\n",
        "listen_freq = 3\n",
        "\n",
        "\n",
        "class DemoBot:\n",
        "    def __init__(self, username, password):\n",
        "        self.agent_details = self.login(username, password)\n",
        "        self.session_token = self.agent_details['sessionToken']\n",
        "        self.chat_state = defaultdict(lambda: {'messages': defaultdict(dict), 'initiated': False, 'my_alias': None})\n",
        "\n",
        "        atexit.register(self.logout)\n",
        "\n",
        "    def listen(self):\n",
        "        while True:\n",
        "            # check for all chatrooms\n",
        "            current_rooms = self.check_rooms(session_token=self.session_token)['rooms']\n",
        "            for room in current_rooms:\n",
        "                # ignore finished conversations\n",
        "                if room['remainingTime'] > 0:\n",
        "                    room_id = room['uid']\n",
        "                    if not self.chat_state[room_id]['initiated']:\n",
        "                        # send a welcome message and get the alias of the agent in the chatroom\n",
        "                        self.post_message(room_id=room_id, session_token=self.session_token, message='Hi, you can send me any message and check if it is echoed in {} seconds.'.format(listen_freq))\n",
        "                        self.chat_state[room_id]['initiated'] = True\n",
        "                        self.chat_state[room_id]['my_alias'] = room['alias']\n",
        "\n",
        "                    # check for all messages\n",
        "                    all_messages = self.check_room_state(room_id=room_id, since=0, session_token=self.session_token)['messages']\n",
        "\n",
        "                    # you can also use [\"reactions\"] to get the reactions of the messages: STAR, THUMBS_UP, THUMBS_DOWN\n",
        "\n",
        "                    for message in all_messages:\n",
        "                        if message['authorAlias'] != self.chat_state[room_id]['my_alias']:\n",
        "\n",
        "                            # check if the message is new\n",
        "                            if message['ordinal'] not in self.chat_state[room_id]['messages']:\n",
        "                                self.chat_state[room_id]['messages'][message['ordinal']] = message\n",
        "                                print('\\t- Chatroom {} - new message #{}: \\'{}\\' - {}'.format(room_id, message['ordinal'], message['message'], self.get_time()))\n",
        "\n",
        "                                ##### You should call your agent here and get the response message #####\n",
        "                                try:\n",
        "                                    m = chatAgent(message['message'])\n",
        "                                    self.post_message(room_id=room_id, session_token=self.session_token, message=m)\n",
        "                                    print(m)\n",
        "                                except:\n",
        "                                    print(\"Something went wrong.\")\n",
        "\n",
        "            time.sleep(listen_freq)\n",
        "\n",
        "    def login(self, username, password):\n",
        "        agent_details = requests.post(url=url + \"/api/login\", json={\"username\": username, \"password\": password}).json()\n",
        "        print('- User {} successfully logged in with session \\'{}\\'!'.format(agent_details['userDetails']['username'], agent_details['sessionToken']))\n",
        "        return agent_details\n",
        "\n",
        "    def check_rooms(self, session_token):\n",
        "        return requests.get(url=url + \"/api/rooms\", params={\"session\": session_token}).json()\n",
        "\n",
        "    def check_room_state(self, room_id, since, session_token):\n",
        "        return requests.get(url=url + \"/api/room/{}/{}\".format(room_id, since), params={\"roomId\": room_id, \"since\": since, \"session\": session_token}).json()\n",
        "\n",
        "    def post_message(self, room_id, session_token, message):\n",
        "        tmp_des = requests.post(url=url + \"/api/room/{}\".format(room_id),\n",
        "                                params={\"roomId\": room_id, \"session\": session_token}, data=message).json()\n",
        "        if tmp_des['description'] != 'Message received':\n",
        "            print('\\t\\t Error: failed to post message: {}'.format(message))\n",
        "\n",
        "    def get_time(self):\n",
        "        return time.strftime(\"%H:%M:%S, %d-%m-%Y\", time.localtime())\n",
        "\n",
        "    def logout(self):\n",
        "        if requests.get(url=url + \"/api/logout\", params={\"session\": self.session_token}).json()['description'] == 'Logged out':\n",
        "            print('- Session \\'{}\\' successfully logged out!'.format(self.session_token))"
      ],
      "metadata": {
        "id": "7QiYbGN1Ju4F"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "username = 'running.hou_bot'\n",
        "password = 'y2trUquIVCl3RQ'\n",
        "demobot = DemoBot(username, password)\n",
        "demobot.listen()"
      ],
      "metadata": {
        "id": "bce5tgazJy6p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "outputId": "b506d033-33c6-4b38-d978-03d1754d95ea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- User running.hou_bot successfully logged in with session 'node0mu2yuis9evenjgmde7jk47oy83295'!\n",
            "\t- Chatroom e5c5592d-aea8-4ee4-98d0-0df789f1667d - new message #1: ' Who is the director of Good Will Hunting?' - 17:11:21, 14-12-2022\n",
            "The relation is director\n",
            "relation is  director\n",
            "KG: I think it is Gus Van Sant\n",
            "\n",
            "----------The answers suggested by embeddings are Gus Van Sant, David Lynch, Scott Patrick Green\n",
            "\t- Chatroom 14160857-15f6-4381-8475-56323cb65214 - new message #1: 'Let me know what Sandra Bullock looks like.' - 17:11:53, 14-12-2022\n",
            "image:3866/rm325111296.jpg\n",
            "\t- Chatroom 6260ff97-7d00-4111-9c4a-04835228130a - new message #0: 'What is the box office of The Princess and the Frog?' - 17:12:51, 14-12-2022\n",
            "The relation is box office\n",
            "relation is  box office\n",
            "The answer is 267000000, according to the crowd, who had an inter-rater agreement of: 0.236.\n",
            "The answer distribution is: 2 support vote and 1 reject vote.\n",
            "----------Embedding: No answer.\n",
            "\t- Chatroom 6260ff97-7d00-4111-9c4a-04835228130a - new message #3: 'Recommend movies similar to Hamlet and Othello.' - 17:13:16, 14-12-2022\n",
            "The English Patient, Romeo and Juliet, Cold Mountain\n",
            "\t- Chatroom 6260ff97-7d00-4111-9c4a-04835228130a - new message #5: 'What is the box office of The Princess and the Frog?' - 17:13:23, 14-12-2022\n",
            "The relation is box office\n",
            "relation is  box office\n",
            "The answer is 267000000, according to the crowd, who had an inter-rater agreement of: 0.236.\n",
            "The answer distribution is: 2 support vote and 1 reject vote.\n",
            "----------Embedding: No answer.\n",
            "\t- Chatroom e5c5592d-aea8-4ee4-98d0-0df789f1667d - new message #3: 'Given that I like The Lion King, Pocahontas, and The Beauty and the Beast, can you recommend some movies?' - 17:14:02, 14-12-2022\n",
            "Aladdin, The Hunchback of Notre Dame, Hercules\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ae90759539c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpassword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'y2trUquIVCl3RQ'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdemobot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDemoBot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdemobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-f0f5ef1432db>\u001b[0m in \u001b[0;36mlisten\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Something went wrong.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlisten_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SEaf5gL6fui3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}