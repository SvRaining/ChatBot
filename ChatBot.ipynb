{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rdflib\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcftqzBGVSGy",
        "outputId": "ff49f4dd-0333-4a1a-ef76-2ace3409403b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.8/dist-packages (6.2.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from rdflib) (3.0.9)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.8/dist-packages (from rdflib) (0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from rdflib) (57.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from isodate->rdflib) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import rdflib\n",
        "from rdflib.namespace import Namespace, RDF, RDFS, XSD\n",
        "from rdflib.term import URIRef, Literal\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from transformers import pipeline\n",
        "import csv\n",
        "from sklearn.metrics import pairwise_distances"
      ],
      "metadata": {
        "id": "yU_EUZObVHFL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WDT = Namespace('http://www.wikidata.org/prop/direct/')\n",
        "\n",
        "\n",
        "def dataloader():\n",
        "    print('############# Start loading data #############')\n",
        "\n",
        "    ner_pipeline = pipeline('ner', model='dbmdz/bert-large-cased-finetuned-conll03-english')\n",
        "\n",
        "    # Build the graph\n",
        "    graph = rdflib.Graph()\n",
        "    graph.parse('data/14_graph.nt', format='turtle')\n",
        "\n",
        "    # Load embedding dictionaries\n",
        "    with open('data/entity_ids.del', 'r') as ifile:\n",
        "        ent2id = {rdflib.term.URIRef(ent): int(idx) for idx, ent in csv.reader(ifile, delimiter='\\t')}\n",
        "        id2ent = {v: k for k, v in ent2id.items()}\n",
        "    with open('data/relation_ids.del', 'r') as ifile:\n",
        "        rel2id = {rdflib.term.URIRef(rel): int(idx) for idx, rel in csv.reader(ifile, delimiter='\\t')}\n",
        "        id2rel = {v: k for k, v in rel2id.items()}\n",
        "\n",
        "    triple_df = pd.read_csv('data/14_graph.tsv', sep='\\t', names=[\"entity1\", \"relation\", \"entity2\"])\n",
        "    entity_emb = np.load('data/entity_embeds.npy')\n",
        "    relation_emb = np.load('data/relation_embeds.npy')\n",
        "\n",
        "    ent2imb = {str(ent): str(imb) for ent, imb in graph.subject_objects(WDT.P345)}\n",
        "\n",
        "    ent2lbl = {ent: str(lbl) for ent, lbl in graph.subject_objects(RDFS.label)}\n",
        "    lbl2ent = {lbl: ent for ent, lbl in ent2lbl.items()}\n",
        "\n",
        "    # Load multimedia dataset\n",
        "    f = open('data/images.json')\n",
        "    mediadata = json.load(f)\n",
        "\n",
        "    # Load crowdsource dataset\n",
        "    crowd_df = pd.read_csv('data/crowd_data.tsv', sep='\\t')\n",
        "\n",
        "    print('Data loading done.')\n",
        "    return graph, ent2id, id2ent, rel2id, id2rel, ent2lbl, lbl2ent, triple_df, entity_emb, \\\n",
        "           relation_emb, ent2imb, mediadata, crowd_df, ner_pipeline"
      ],
      "metadata": {
        "id": "yuTmN6IYVkIE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4yaUYxC5VDqH"
      },
      "outputs": [],
      "source": [
        "def questionprocessor(question, ner_pipeline):\n",
        "    if question.find('VI -') != -1:\n",
        "        question = question.replace('-', 'â€“')\n",
        "\n",
        "    if question.find('ecommend') != -1:\n",
        "        qtype = 'Recommend'\n",
        "    elif (question.find('picture') != -1) or (question.find('like') != -1) or (question.find('figure') != -1):\n",
        "        qtype = 'Multimedia'\n",
        "    else:\n",
        "        print('Please choose a question type: 1. KG 2. Embedding')\n",
        "        tmp = input()\n",
        "        if tmp == '1':\n",
        "            qtype = 'KG'\n",
        "        else:\n",
        "            qtype = 'Embedding'\n",
        "\n",
        "    if question.find('of') != -1:\n",
        "        sub1 = \"of \"\n",
        "        sub2 = \" ?\"\n",
        "        idx1 = question.find(sub1)\n",
        "        idx2 = question.find(sub2)\n",
        "        movie = question[idx1 + len(sub1): idx2]\n",
        "        return qtype, movie\n",
        "    else:\n",
        "        movies = []\n",
        "        entities = ner_pipeline(question, aggregation_strategy=\"simple\")\n",
        "        for entity in entities:\n",
        "            movies.append(entity['word'])\n",
        "\n",
        "    if question.find('ecommend') != -1:\n",
        "        return qtype, movies\n",
        "    else:\n",
        "        return qtype, movies[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def factual(question, graph, movies, ent2lbl, lbl2ent, ans_df):\n",
        "    WD = Namespace('http://www.wikidata.org/entity/')\n",
        "    WDT = Namespace('http://www.wikidata.org/prop/direct/')\n",
        "\n",
        "    if question.find('of') != -1:\n",
        "        sub1 = \" the \"\n",
        "        sub2 = \" of \"\n",
        "        idx1 = question.find(sub1)\n",
        "        idx2 = question.find(sub2)\n",
        "        relation = question[idx1 + len(sub1): idx2]\n",
        "        print('The relation is', relation)\n",
        "\n",
        "    if question.find('direct') != -1:\n",
        "        relation = 'director'\n",
        "\n",
        "    query_relURI = '''\n",
        "        SELECT ?rel WHERE{{\n",
        "            ?rel rdfs:label \"{}\"@en.\n",
        "            }}'''.format(relation) \n",
        "\n",
        "    relURIList = list(graph.query(query_relURI))\n",
        "    for idx, relURI in enumerate(relURIList):\n",
        "        rel_tmp = relURI[0].n3()\n",
        "        if WDT in rel_tmp:\n",
        "            rel = rel_tmp\n",
        "\n",
        "    mov = lbl2ent[movies].n3()\n",
        "\n",
        "    ent1 = re.sub('<|>', '', mov)\n",
        "    ent2 = re.sub('<|>', '', rel)\n",
        "    crowd_idx1 = 'wd:'+re.findall(r'http://www.wikidata.org/entity/(.*)', ent1)[0]\n",
        "    crowd_idx2 = 'wdt:'+re.findall(r'http://www.wikidata.org/prop/direct/(.*)', ent2)[0]\n",
        "    \n",
        "    if (crowd_idx1 in ans_df['Input1ID'].values) & (crowd_idx2 in ans_df['Input2ID'].values):\n",
        "        tmp = ans_df.loc[ans_df['Input1ID']==crowd_idx1]\n",
        "        ans = tmp['Input3ID'].values[0]\n",
        "        if ans.startswith('wd:'):\n",
        "            ans = ent2lbl[rdflib.term.URIRef(ent1)]\n",
        "        print('The answer is',ans+', according to the crowd, who had an inter-rater agreement of:', tmp['Kappa'].values[0])\n",
        "        print('The answer distribution is:', tmp['Correct'].values[0],'support vote and', (3-tmp['Correct'].values[0]), 'reject vote.')\n",
        "\n",
        "    else:\n",
        "        # rel = '<'+rel+'>'\n",
        "\n",
        "        idxs = triple_df[(triple_df['entity1'] == mov) & (triple_df['relation'] == rel)].index.values\n",
        "\n",
        "        entity2 = triple_df['entity2'].iloc[idxs[0]]\n",
        "        entity2 = re.sub('<|>', '', entity2)\n",
        "        entity2_lbl = ent2lbl[rdflib.term.URIRef(entity2)]\n",
        "\n",
        "        answers = [\n",
        "            'A: I think it is ' + entity2_lbl,\n",
        "            'A: ' + entity2_lbl + ' is the ' + relation + ' of ' + movies\n",
        "        ]\n",
        "\n",
        "        print(random.choice(answers))"
      ],
      "metadata": {
        "id": "-a-wFnxdVnDs"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding(question, graph, movies, ent2id, id2ent, rel2id, id2rel, triple_df, entity_emb, relation_emb):\n",
        "    WDT = rdflib.Namespace('http://www.wikidata.org/prop/direct/')\n",
        "\n",
        "    if question.find('the') != -1:\n",
        "        sub1 = \" the \"\n",
        "        sub2 = \" of \"\n",
        "        idx1 = question.find(sub1)\n",
        "        idx2 = question.find(sub2)\n",
        "        relation = question[idx1 + len(sub1): idx2]\n",
        "        print('relation is ', relation)\n",
        "\n",
        "    if question.find('direct') != -1:\n",
        "        relation = 'director'\n",
        "\n",
        "    query_relURI = '''\n",
        "        SELECT ?rel WHERE{{\n",
        "            ?rel rdfs:label \"{}\"@en.\n",
        "            }}'''.format(relation)\n",
        "\n",
        "    relURI = []\n",
        "    relURIList = list(graph.query(query_relURI))\n",
        "    for idx, relURI in enumerate(relURIList):\n",
        "        tmp = str(relURI[0])\n",
        "        if WDT in tmp:\n",
        "            rel = tmp\n",
        "\n",
        "    mov = str(lbl2ent[movies].n3())\n",
        "\n",
        "    rel_id = rel2id[rdflib.term.URIRef(rel)]\n",
        "    mov_id = ent2id[rdflib.term.URIRef(re.sub('<|>','',mov))]\n",
        "\n",
        "    rel = '<'+rel+'>'\n",
        "\n",
        "    topN = 3\n",
        "    rel_emb = np.atleast_2d(relation_emb[rel_id])\n",
        "    rel_dist = pairwise_distances(rel_emb, relation_emb)\n",
        "    relation2 = []\n",
        "    for idx in rel_dist.argsort().reshape(-1)[:3]:\n",
        "        relation2.append(str(id2rel[idx].n3()))\n",
        "\n",
        "    idxs = 0\n",
        "    idxs = triple_df[(triple_df['entity1'] == mov) & (triple_df['relation'] == rel)].index.values\n",
        "    length = len(idxs)\n",
        "    if length == 0:\n",
        "        idxs = triple_df[(triple_df['entity1'] == mov) & (triple_df['relation'] == relation2[1])].index.values\n",
        "\n",
        "    entity2 = triple_df['entity2'].iloc[idxs[0]]\n",
        "    entity2 = re.sub('<|>','',entity2)\n",
        "    entity2_id = ent2id[rdflib.term.URIRef(entity2)]\n",
        "\n",
        "    # TransE\n",
        "    topN = 3\n",
        "    emb = np.atleast_2d(entity_emb[entity2_id])\n",
        "    dist = pairwise_distances(emb, entity_emb)\n",
        "    entity2 = []\n",
        "    for idx in dist.argsort().reshape(-1)[:3]:\n",
        "        print(ent2lbl[id2ent[idx]])"
      ],
      "metadata": {
        "id": "GVaMo0JKVsfm"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multimedia(graph, movies, mediadata, ner_pipeline):\n",
        "    WDT = Namespace('http://www.wikidata.org/prop/direct/')\n",
        "    lbl2ent = {str(lbl): str(ent) for ent, lbl in graph.subject_objects(RDFS.label)}\n",
        "    ent2imb = {str(ent): str(imb) for ent, imb in graph.subject_objects(WDT.P345)}\n",
        "\n",
        "    entities = ner_pipeline(question, aggregation_strategy=\"simple\")\n",
        "    for entity in entities:\n",
        "        lbl = entity['word']\n",
        "\n",
        "    ent = lbl2ent[lbl]\n",
        "    imb = ent2imb[ent]\n",
        "\n",
        "    if imb[:2] == 'tt':\n",
        "        for item in mediadata:\n",
        "            if imb in item['movie']:\n",
        "                print(item['img'])\n",
        "                break\n",
        "    elif imb[:2] == 'nm':\n",
        "        for item in mediadata:\n",
        "            if (imb in item['cast']) & (len(item['cast'])==1):\n",
        "                print(item['img'])\n",
        "                break\n",
        "    else:\n",
        "        print('Not a movie or human.')"
      ],
      "metadata": {
        "id": "4z11LYUwehoq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend(question, graph, movies, ent2lbl, triple_df):\n",
        "    mov_list = []\n",
        "\n",
        "    mov_list = []\n",
        "    for mov in movies:\n",
        "        mov_lbl = [str(s) for s,  in graph.query('''\n",
        "            SELECT ?movie WHERE {\n",
        "                ?movie rdfs:label '%s'@en .\n",
        "            }'''%mov)]\n",
        "        if len(mov_lbl)!= 0:\n",
        "            mov_list.append(mov_lbl)\n",
        "    \n",
        "    for i in range(len(mov_list)):\n",
        "        for j in range(len(mov_list[i])):\n",
        "            mov_list[i][j] = '<' + mov_list[i][j] + '>'\n",
        "\n",
        "    dfs = []\n",
        "    for i in range(len(mov_list)):\n",
        "        df = []\n",
        "        df = triple_df.loc[triple_df['entity1'].isin(mov_list[i])]\n",
        "        dfs.append(df)\n",
        "\n",
        "    for i in range(1, len(mov_list)):\n",
        "        dfs[i] = pd.merge(dfs[i-1], dfs[i], on=[\"relation\", \"entity2\"])\n",
        "    \n",
        "    rel_df = dfs[len(mov_list)-1]\n",
        "    rel_df = rel_df.drop_duplicates(subset=['relation', 'entity2'])\n",
        "    entity2 = rel_df['entity2'].values.tolist()\n",
        "\n",
        "    common = []\n",
        "    for ent in entity2:\n",
        "        if '<' in ent:\n",
        "            ent = re.sub('<|>','',ent)\n",
        "            lbl = ent2lbl[rdflib.term.URIRef(ent)]\n",
        "        else:\n",
        "            lbl = ent\n",
        "        common.append(lbl)\n",
        "\n",
        "    entity1 = []\n",
        "    entity1 = triple_df['entity1'].loc[triple_df['entity2'].isin(entity2)]  \n",
        "    entity1 = entity1.value_counts()[len(mov_list):len(mov_list)+3].index.tolist()  \n",
        "\n",
        "    answers = []\n",
        "    for ent in entity1:\n",
        "        ent = re.sub('<|>','',ent)\n",
        "        lbl = ent2lbl[rdflib.term.URIRef(ent)]\n",
        "        answers.append(lbl)\n",
        "\n",
        "    print(answers[0]+', '+answers[1]+', '+answers[2])"
      ],
      "metadata": {
        "id": "uFhgBp5Zi0fp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crowdsource(crowd_df):\n",
        "\n",
        "    crowd_df.drop(['Title','Reward','AssignmentId','AssignmentStatus'], inplace=True, axis=1)\n",
        "    crowd_df['LifetimeApprovalRate'] = crowd_df['LifetimeApprovalRate'].str.rstrip('%').astype('float') / 100.0\n",
        "    crowd_df = crowd_df.loc[(crowd_df['WorkTimeInSeconds'] >= 50) & (crowd_df['LifetimeApprovalRate'] >= 0.7)]\n",
        "    crowd_df.drop(['WorkerId','WorkTimeInSeconds','LifetimeApprovalRate'], inplace=True, axis=1)\n",
        "    ans_df = crowd_df.groupby(['HITId']).first()\n",
        "\n",
        "    rate = []\n",
        "    ans_df['Correct'] = None\n",
        "\n",
        "    for i in range(1, len(ans_df)+1):\n",
        "        # Get the specific group\n",
        "        df = crowd_df.loc[crowd_df['HITId']== i]\n",
        "\n",
        "        corr_count = int(df['AnswerID'][df['AnswerID']==1].count())\n",
        "        incorr_count = int(df['AnswerID'][df['AnswerID']==2].count())\n",
        "        ans_df['Correct'][i] = corr_count\n",
        "\n",
        "\n",
        "        rate.append([corr_count, incorr_count])\n",
        "        if (corr_count < incorr_count):\n",
        "            ans_df['AnswerLabel'][i] = 'INCORRECT'\n",
        "\n",
        "            fixValueLoc = df['FixValue'].first_valid_index()\n",
        "            fixPositionLoc = df['FixPosition'].first_valid_index()\n",
        "\n",
        "            if fixValueLoc is not None:\n",
        "\n",
        "                fixPosition = crowd_df['FixPosition'][fixPositionLoc]\n",
        "                fixValue = crowd_df['FixValue'][fixValueLoc]\n",
        "\n",
        "                if fixPosition == 'Subject':\n",
        "                    if fixValue.startswith('Q'):\n",
        "                        ans_df['Input1ID'][i] = 'wd:'+fixValue\n",
        "                    else:\n",
        "                        ans_df['Input1ID'][i] = fixValue\n",
        "                elif fixPosition == 'Predicate':\n",
        "                    if fixValue.startswith('P'):\n",
        "                        ans_df['Input2ID'][i] = 'wdt:'+fixValue\n",
        "                    else:\n",
        "                        ans_df['Input2ID'][i] = fixValue\n",
        "                else:\n",
        "                    if fixValue.startswith('Q'):\n",
        "                        ans_df['Input3ID'][i] = 'wd:'+fixValue\n",
        "                    else:\n",
        "                        ans_df['Input3ID'][i] = fixValue\n",
        "        \n",
        "        else:\n",
        "            ans_df['AnswerLabel'][i] = 'CORRECT'\n",
        "\n",
        "    def checkInput(rate, n):\n",
        "        \"\"\" \n",
        "        Check correctness of the input matrix\n",
        "        @param rate - ratings matrix\n",
        "        @return n - number of raters\n",
        "        @throws AssertionError \n",
        "        \"\"\"\n",
        "        N = len(rate)\n",
        "        k = len(rate[0])\n",
        "        assert all(len(rate[i]) == k for i in range(k)), \"Row length != #categories)\"\n",
        "        assert all(isinstance(rate[i][j], int) for i in range(N) for j in range(k)), \"Element not integer\" \n",
        "        assert all(sum(row) == n for row in rate), \"Sum of ratings != #raters)\"\n",
        "\n",
        "    def fleissKappa(rate,n):\n",
        "        \"\"\" \n",
        "        Computes the Kappa value\n",
        "        @param rate - ratings matrix containing number of ratings for each subject per category \n",
        "        [size - N X k where N = #subjects and k = #categories]\n",
        "        @param n - number of raters   \n",
        "        @return fleiss' kappa\n",
        "        \"\"\"\n",
        "\n",
        "        N = len(rate)\n",
        "        k = len(rate[0])\n",
        "        print(\"#raters = \", n, \", #subjects = \", N, \", #categories = \", k)\n",
        "        checkInput(rate, n)\n",
        "\n",
        "        #mean of the extent to which raters agree for the ith subject \n",
        "        PA = sum([(sum([i**2 for i in row])- n) / (n * (n - 1)) for row in rate])/N\n",
        "        print(\"PA = \", PA)\n",
        "        \n",
        "        # mean of squares of proportion of all assignments which were to jth category\n",
        "        PE = sum([j**2 for j in [sum([rows[i] for rows in rate])/(N*n) for i in range(k)]])\n",
        "        print(\"PE =\", PE)\n",
        "        \n",
        "        kappa = -float(\"inf\")\n",
        "        try:\n",
        "            kappa = (PA - PE) / (1 - PE)\n",
        "            kappa = float(\"{:.3f}\".format(kappa))\n",
        "        except ZeroDivisionError:\n",
        "            print(\"Expected agreement = 1\")\n",
        "\n",
        "        print(\"Fleiss' Kappa =\", kappa)\n",
        "        \n",
        "        return kappa\n",
        "\n",
        "    len1 = len(ans_df[ans_df['HITTypeId']=='7QT'])\n",
        "    len2 = len(ans_df[ans_df['HITTypeId']=='8QT'])\n",
        "    len3 = len(ans_df[ans_df['HITTypeId']=='9QT'])\n",
        "\n",
        "    rate1 = rate[:len1]\n",
        "    rate2 = rate[len1:len1+len2]\n",
        "    rate3 = rate[len1+len2:]\n",
        "\n",
        "    kappa1 = fleissKappa(rate1, 3)\n",
        "    kappa2 = fleissKappa(rate2, 3)\n",
        "    kappa3 = fleissKappa(rate3, 3)\n",
        "\n",
        "    ans_df['Kappa'] = None\n",
        "    ans_df['Kappa'][:len1] = kappa1\n",
        "    ans_df['Kappa'][len1:len1+len2] = kappa2\n",
        "    ans_df['Kappa'][len1+len2:] = kappa3\n",
        "\n",
        "    return ans_df"
      ],
      "metadata": {
        "id": "wwIFNBUb9ZG_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph, ent2id, id2ent, rel2id, id2rel, ent2lbl, lbl2ent, triple_df, \\\n",
        "entity_emb, relation_emb, ent2imb, mediadata, crowd_df, ner_pipeline = dataloader()\n",
        "ans_df = crowdsource(crowd_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-A37Jycl6b2",
        "outputId": "7ff72f8a-0fd0-4d5c-e28f-b9c1b1277149"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############# Start loading data #############\n",
            "Data loading done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = input('Q: ')\n",
        "qtype, movies = questionprocessor(question, ner_pipeline)\n",
        "\n",
        "if qtype == 'KG':\n",
        "    factual(question, graph, movies, ent2lbl, lbl2ent, ans_df)\n",
        "elif qtype == 'Embedding':\n",
        "    embedding(question, graph, movies, ent2id, id2ent, rel2id, id2rel, triple_df, entity_emb, relation_emb)\n",
        "elif qtype == 'Multimedia':\n",
        "    multimedia(graph, movies, mediadata, ner_pipeline)\n",
        "elif qtype == 'Recommend':\n",
        "    recommend(question, graph, movies, ent2lbl, triple_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbbvD9-lVpW0",
        "outputId": "6f672060-c5ff-4cdd-a9f6-a91340a78c5d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Who is the executive producer of X-Men: First Class?\n",
            "Please choose a question type: 1. KG 2. Embedding\n",
            "1\n",
            "The relation is executive producer\n",
            "The answer is X-Men: First Class, according to the crowd, who had an inter-rater agreement of: 0.263\n",
            "The answer distribution is: 2 support vote and 1 reject vote.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XjN8_xTAkzoF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}